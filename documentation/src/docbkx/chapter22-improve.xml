<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	id="improve" xmlns="http://www.oasis-open.org/docbook/4.5" xmlns:xl="http://www.w3.org/1999/xlink"
	xsi:schemaLocation="http://www.oasis-open.org/docbook/4.5 http://www.oasis-open.org/docbook/xsd/4.5/docbook.xsd">

	<title>Improve</title>

	<chapterinfo>
		<abstract>
			<para>The functions in the 'Improve' tree are all what we would
				describe as first class 'Data Quality functions'. They not only
				analyze a problem but also usually present a solution to it.
			</para>
		</abstract>
	</chapterinfo>

	<section id="duplicate_detection">
		<title>
			Duplicate detection
			<inlinemediaobject>
				<imageobject>
					<imagedata fileref="notice_commercial_editions_only.png"
						format="PNG" />
				</imageobject>
			</inlinemediaobject>
		</title>

		<para>The 'Duplicate detection' function allows you to do fuzzy
			matching of duplicate
			records
			- records that represent the same person,
			organization, product or
			other entity.
		</para>

		<mediaobject>
			<imageobject>
				<imagedata fileref="analyzer_reference_duplicate_detection_menu.jpg"
					format="JPG" />
			</imageobject>
		</mediaobject>

		<para>The main characteristics of the Duplicate detection
			function is:
		</para>

		<orderedlist>
			<listitem>
				<para>
					<emphasis>High Quality</emphasis>
					- Quality is the hallmark of matching, our duplicate detection
					feature delivers on this promise.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>Scalable</emphasis>
					- For large datasets Duplicate detection leverages the Hadoop
					framework for practically unlimited scalability.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>Fast and interactive</emphasis>
					- On a single machine you can work quickly and interactively to
					refine your duplicate detection model.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>International</emphasis>
					- International data is supported and no regional knowledge has
					been encoded into the deduplication engine - you provide the
					business rules externally.
				</para>
			</listitem>
			<listitem>
				<para>
					<emphasis>Machine Learning based</emphasis>
					- The Duplicate detection engine is configured by examples. During
					a series of training sessions you can refine the deduplication
					model simply by having a conversation with the tool about what is
					and what isn't a good example of a duplicate.
				</para>
			</listitem>
		</orderedlist>

		<tip>
			<para>Duplicate detection does work fine with raw data. But if you
				have dirty data and the way data is registered has a lot of
				variance, we suggest you first do your best to standardize the data
				before finding duplicates.
			</para>
			<para>Standardization can be made by trimming, tokenizing, removing
				unwanted characters, replace synonyms and things like that. Explore
				the transformations available in DataCleaner in order to get your
				data cleansed before trying to deduplicate it.
			</para>
		</tip>

		<para>In the following sections we will walk through how to use the
			'Duplicate detection' function. The function has three modes: Model
			training, Detection and Untrained detection.
		</para>

		<section>
			<title>
				'Model training' mode
			</title>

			<para>
				In the Model training mode the user of Duplicate detection is
				looking to
				<emphasis>train</emphasis>
				the Machine Learning engine. When running your job in Model training
				mode you will be shown a
				number of potential duplicate record pairs,
				and determine if they
				are duplicate or not.
			</para>

			<para>To start the Training mode, simply add the
				function and select
				the columns that you wish to use for matching.
				Additionally you might
				wish to configure:
			</para>

			<table>
				<title>Model training properties</title>
				<tgroup cols="2">
					<thead>
						<row>
							<entry>Property</entry>
							<entry>Description</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Max records for training</entry>
							<entry>The Training tool will keep a random sample of the dataset
								in memory to provide as training input, and an interactive user
								experience. This number determines how many records will be
								selected for this sample.
							</entry>
						</row>
						<row>
							<entry>Key column</entry>
							<entry>If your dataset has a unique key, we encourage you to
								select it using this property. Configuring the key column has
								the benefit that if you wish to export a training reference
								later, it can be re-applied very easily.
							</entry>
						</row>
						<row>
							<entry>Reference file</entry>
							<entry>If you previously created a reference file in the Training
								tool that you wish to reuse, you can select it here.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>

			<para>In contrast to most other analyzers in DataCleaner which shows
				a result screen after execution, the Training mode opens a new
				dialog when started. The training tool dialog allows users to train
				matching models. The top of the dialog contains a button bar. Below
				the button bar, the training tool shows some tab buttons. By default
				the potential duplicates will be shown. For each potential duplicate
				you can toggle the button on the right side to determine if the pair
				is a duplicate or not:
			</para>

			<mediaobject>
				<imageobject>
					<imagedata fileref="analyzer_reference_training_tool.jpg"
						format="JPG" />
				</imageobject>
			</mediaobject>

			<para>You do not need to classify all samples shown. Recommended
				usage is:
			</para>

			<orderedlist>
				<listitem>
					<para>Classify at least 20-30 duplicate pairs or more (more is
						better)
					</para>
				</listitem>
				<listitem>
					<para>Classify at least 20-30 unique records or more (more is
						better)
					</para>
				</listitem>
			</orderedlist>

			<para>Once you've classified records you can press the 'Train model'
				button in the upper right corner. This will refine the matching
				model and present a new set of interesting potential duplicates. You
				can continue this way and quite quickly have classified the required
				amount of pairs.
			</para>

			<para>
				All duplicate detection models may have irregularities. When
				you ask a computer to do a complex task like matching, it may come
				up with a model that has slight differences from your
				classifications. You can inspect the current model's differences
				from your classifications in the tab 'Discrepancies'.
			</para>

			<para>
				Every time you classify a duplicate, it is added to the
				<emphasis>reference</emphasis>
				of the Training session. You can inspect your complete
				reference in
				the tab 'Duplicates reference'.
			</para>

			<para>If you're looking for particular types of duplicate pairs, you
				may want to go to the 'Search pairs' tab. In this tab you will find
				options to search for records with matching or non-matching values
				for particular fields. This may be a very useful shortcut for
				finding proper duplicate examples.
			</para>

			<para>Finally, the tab 'Training parameters' presents buttons and
				sliders for
				you to influence the Machine Learning approach. It
				presents the
				following options:
			</para>

			<table>
				<title>'Training parameters' options</title>
				<tgroup cols="2">
					<thead>
						<row>
							<entry>Parameter</entry>
							<entry>Description</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Only optimize the preselection when sufficient duplicates
								have been marked
							</entry>
							<entry>This will optimize the matching rules using an initial
								model for pre-selection until sufficient pairs have been marked
								as duplicates and uniques. This is the default value.
							</entry>
						</row>
						<row>
							<entry>Always optimize the preselection</entry>
							<entry>This will always train pre-selection and matching rules.
								Training both is slower, but an improved preselection has a
								better performance during
								duplicate detection. Training the
								potential pair generation too early
								might result in the exclusion
								of some groups of duplicate pairs.
								It is recommended to perform
								training of the full model only after the user has
								classified
								varying examples of duplicates and unique records.
							</entry>
						</row>
						<row>
							<entry>Never optimize the preselection</entry>
							<entry>This never trains the pre-selection, but only the matching
								rules.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>

			<para>
				After updating the matching model, the user can continue in 2
				ways. If
				the user is satisfied with the model (few false positives
				and false
				negatives) then he can save the model and start using it in
				duplicate detection. Otherwise, the user must start a new iteration
				of classifying
				more of the presented samples and refining the model
				again.
			</para>
			<para>
				Longer training set typically allows for a more advanced
				matching model,
				capable of handling more corner cases. The false
				negatives and
				false positives lists give a good impression of the
				current state of the
				matching model. The user should continue
				training until the
				differences
				in these lists are acceptable.
			</para>
			<para>
				When finishing the training, save the reference to file. The
				reference
				is the most important part of the Duplicate detection
				model, since it allows you to reproduce and re-train a model based
				on past classifications.
			</para>

			<para>To validate the training results and obtain the best model,
				training can be repeated on a different sample.
			</para>

			<orderedlist>
				<listitem>
					<para>
						Save the reference
					</para>
				</listitem>
				<listitem>
					<para>Close the training tool.</para>
				</listitem>
				<listitem>
					<para>In the Training tool properties, specify the saved reference.
					</para>
				</listitem>
				<listitem>
					<para>Re-run the Training tool. A new sample will be generated. All
						marked pairs in the saved reference are automatically included in
						the new sample.
					</para>
				</listitem>
				<listitem>
					<para>Press the 'Train model' button in the Training tool. This
						will train a model on the existing reference.
					</para>
				</listitem>
				<listitem>
					<para>You can view the discrepancies (false positives, false
						negatives) of the trained model against the records in the new
						sample.
					</para>
				</listitem>
				<listitem>
					<para>You can review the potential duplicates to determine if a
						category of duplicates is missing.
					</para>
				</listitem>
				<listitem>
					<para>Add more pairs to the reference as needed.</para>
				</listitem>
			</orderedlist>

			<para>Use the 'Export model' button to save the current matching
				model. A dialog appears which allows the user to choose the location
				and file name for the model.
			</para>

		</section>

		<section>
			<title>
				'Detection' mode
			</title>
			<para>When the matching model is complete you are ready to find all
				the duplicates in the dataset, usie the Duplicate Detection
				analyzer.
			</para>

			<para>Configure the analyzer with the same input fields as your
				Training tool. Load the matching model from the file you saved at
				the end of your training session. When you run the job, you will see
				a Duplicate detection result with complete groups of duplicates,
				like this:
			</para>

			<mediaobject>
				<imageobject>
					<imagedata fileref="analyzer_reference_duplicate_detection.jpg"
						format="JPG" />
				</imageobject>
			</mediaobject>

			<para>Once you have a duplicate detection result that you wish to
				post-process, e.g. for merging or manual inspection, you can export
				the result by clicking one of the 'Export to staging table' or
				'Export to Excel' buttons in the top of the result screen.
			</para>

			<para>The Duplicate detection analyzer can run stand-alone to
				find
				duplicates in datasets up to a half to 1 million records
				(depending
				on the amount of columns). For larger datasets, the
				Duplicate
				detection component can be used in combination with an
				Hadoop server
				component. This server component is a Enterprise
				edition feature
				only.
			</para>

		</section>

		<section>
			<title>
				'Untrained detection' mode
			</title>
			<para>Finally, there's also a 'Untrained detection' mode. This allows
				you to skip 'Model training' and just ask the application to do it's
				best effort without any proper model. This mode is not recommended
				for production use and is considered 'experimental', but may provide
				a great quick impression of some of the duplicates you have in your
				dataset.
			</para>
		</section>

	</section>

	<section id="merge_duplicates">

		<title>
			Merge duplicates
			<inlinemediaobject>
				<imageobject>
					<imagedata fileref="notice_commercial_editions_only.png"
						format="PNG" />
				</imageobject>
			</inlinemediaobject>
		</title>

		<para>Merging duplicates is the next step after detecting them in a
			dataset. It helps to restore a single version of truth by combining
			information from all the duplicate records representing the same
			physical entity.
		</para>

		<para>
			In this section, we assume that the steps from previous section
			"Duplicate detection" are completed and the duplicates are available
			in a staging table "Duplicates export".
		</para>

		<section>
			<title>
				Copy the unique records
			</title>

			<para>
				Before we get into merging the duplicates, let's make sure all
				the unique entries in our datasets are propagated to the new
				(cleansed) dataset.
			</para>

			<orderedlist numeration="arabic">
				<listitem>
					Open the original dataset
				</listitem>
				<listitem>
					Apply "Table lookup" transformer to
					it. Configure "Table lookup" to
					check if a record is present in the
					results of duplicate detection
					staging table (lookup in RECORDS
					table). Choose a unique identifier
					column as condition value (in
					this case: CUSTOMERNUMBER). As an
					output column choose GROUPID, which is an additional
					information in
					RECORDS table.
					<mediaobject>
						<imageobject>
							<imagedata fileref="merge_duplicates_table_lookup_properties.png"
								format="PNG" />
						</imageobject>
					</mediaobject>
				</listitem>
				<listitem>
					Table lookup will output the value of GROUPID or NULL. We are
					interested in NULL values as it means the records has not been
					found
					in duplicates table, so this is the unique record we would
					like to
					transfer to the final file. Therefore, we add Null check
					filter to
					save only the unique file (GROUPID is equal to NULL).
					<mediaobject>
						<imageobject>
							<imagedata fileref="merge_duplicates_null_check.png"
								format="PNG" />
						</imageobject>
					</mediaobject>
				</listitem>
				<listitem>
					Save the output of the filter to a datastore of your choice (CSV
					file, Excel file, database table etc.). The whole job should look
					like:
					<mediaobject>
						<imageobject>
							<imagedata fileref="merge_duplicates_unique_job.png"
								format="PNG" />
						</imageobject>
					</mediaobject>
				</listitem>
			</orderedlist>

		</section>

		<section>
			<title>
				Merge duplicates
			</title>

			<para>Now it is time to merge the duplicates into one record in our
				final file.
			</para>

			<para>In the Improve menu, in the
				Deduplication submenu, there are two
				components that are useful for
				this task - Merge duplicates (simple)
				and Merge duplicates (advanced).
				The simple version just picks one
				record from the group of duplicates
				while the advanced one enables
				the
				user to combine records, taking
				some values from one record, some
				from another. In this example, we
				will use the simple version.
			</para>

			<orderedlist>
				<listitem>
					Open the duplicates staging table as the job source datastore
				</listitem>
				<listitem>
					Apply "Merge duplicates (Simple)" transformer to the RECORDS table
					from the duplicates datastore. Select all the relevant columns as
					input (in our case all of them) and select GROUPID column in "Group
					id" dropdown menu and GROUPSIZE column in "Group count".
					<mediaobject>
						<imageobject>
							<imagedata fileref="merge_duplicates_merge_simple_properties.png"
								format="PNG" />
						</imageobject>
					</mediaobject>
				</listitem>
				<listitem>
					Merge duplicates (Simple) transformer will output all the input
					columns as output + additional colums carrying metadata about the
					record. One of them is "Merge status" that can have two possible
					values: SURVIVOR and NON_SURVIVOR. SURVIVOR indicates the record
					that has been chosen as the final one. Let's add an Equals filter
					to the job in order to
					write only these SURVIVOR records into our
					final file.

					<mediaobject>
						<imageobject>
							<imagedata fileref="merge_duplicates_equals_filter_properties.png"
								format="PNG" />
						</imageobject>
					</mediaobject>
				</listitem>
				<listitem>
					Save the output of the filter to a file of your choice (choose the
					same type as for the unique records file in previous section). The
					whole job graph should look similar to this:
					<mediaobject>
						<imageobject>
							<imagedata fileref="merge_duplicates_merge_job.png"
								format="PNG" />
						</imageobject>
					</mediaobject>
				</listitem>
			</orderedlist>

		</section>

		Following above sections, we obtained a datastore with unique values
		and a datastore with duplicates merged into a single record. The final
		step is to merge these two files together.

	</section>

	<section id="synonym_lookup_transformer">
		<title>Synonym lookup</title>
		<para>
			The Synonym lookup transformation is a critical part of DataCleaner's
			ability to standardize and cleanse data. Using this component you can
			look up values in a
			<link linkend="synonym_catalogs">synonym catalog</link>
			and have it replaced with it's master term, if it is found to be a
			synonym.
		</para>
		<para>Below is a screenshot of the synonym lookup's configuration
			panel:
		</para>
		<mediaobject>
			<imageobject>
				<imagedata fileref="transformer_reference_synonym_lookup.png"
					format="PNG" scalefit="1" />
			</imageobject>
		</mediaobject>
		<para>
			The configuration of the Synonym lookup is simple:
		</para>
		<orderedlist>
			<listitem>
				<para>Select the column to apply the lookup function to.</para>
			</listitem>
			<listitem>
				<para>Use the 'Retain original value' option to determine if
					unmatched values (non-synonyms) should be retained or if a null
					value should be returned if there is no match.
				</para>
			</listitem>
			<listitem>
				<para>Select the synonym catalog to use for lookups.</para>
			</listitem>
		</orderedlist>

		<para>
			If your synonym catalog contains all the allowed values for a
			particula column, it can be a good idea to uncheck the 'Retain
			original value' checkbox and then do a simple null-check on the
			resulting output column. If null values are found, it's because there
			are values in the column that the synonym catalog is not able to
			standardize.
		</para>
	</section>

	<section id="DE_suppression">
		<title>
			DE Address Correction/Suppression
			<inlinemediaobject>
				<imageobject>
					<imagedata fileref="notice_commercial_editions_only.png"
						format="PNG" />
				</imageobject>
			</inlinemediaobject>
		</title>
		<para>This component provides Address Correction and Suppression
			services for Germany. Use it to check that the name and address data
			you have about people is up to date and correct. The following
			Address suppression checks currently exist:
		</para>
		<orderedlist>
			<listitem>
				<para>Change of Address (with new address) check</para>
			</listitem>
			<listitem>
				<para>Moved Away (without new address) check</para>
			</listitem>
			<listitem>
				<para>Deceased check</para>
			</listitem>
		</orderedlist>
		<tip>
			<para>Customers need a set of credentials to access the service.
			</para>
		</tip>
		<section>
			<title>
				Address and suppression data sources
			</title>
			<para>The service combines data of several sources, including
				Deutsche
				Post.
			</para>
		</section>
		<section>
			<title>
				Output
			</title>
			<para>This component outputs name and address fields, in addition to
				the following indicators:
			</para>

			<table>
				<title>DE Address Correction/Suppression output</title>
				<tgroup cols="2">
					<thead>
						<row>
							<entry>Output column</entry>
							<entry>Description</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Address validation status</entry>
							<entry>
								The status code of the record. Following values are possible:
								<orderedlist>
									<listitem>
										Valid
									</listitem>
									<listitem>
										Corrected to valid
									</listitem>
									<listitem>
										Ambigiuous
									</listitem>
									<listitem>
										Invalid
									</listitem>
									<listitem>
										Not processed
									</listitem>
									<listitem>
										Failure
									</listitem>
									<listitem>
										Unknown
									</listitem>
								</orderedlist>
							</entry>
						</row>
						<row>
							<entry>Is moved?</entry>
							<entry>A boolean flag indicating if the person has moved to a
								new
								address.
							</entry>
						</row>
						<row>
							<entry>Is moved to unknown address?</entry>
							<entry>A boolean flag indicating if the person has moved away
								(with no known new address).
							</entry>
						</row>
						<row>
							<entry>Is deceased?</entry>
							<entry>A boolean flag indicating if the person is deceased.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>
		</section>
	</section>

	<section id="UK_suppression">
		<title>
			UK Address Correction/Suppression
			<inlinemediaobject>
				<imageobject>
					<imagedata fileref="notice_commercial_editions_only.png"
						format="PNG" />
				</imageobject>
			</inlinemediaobject>
		</title>
		<para>This component provides Address Correction and Suppression
			services for the United Kingdom / Great Britain. Use it to check that
			the name and address data you have about people is up to date and
			correct. The following Address suppression checks currently exist:
		</para>
		<orderedlist>
			<listitem>
				<para>Change of Address (with new address) check</para>
			</listitem>
			<listitem>
				<para>Moved Away (without new address) check</para>
			</listitem>
			<listitem>
				<para>Deceased check</para>
			</listitem>
			<listitem>
				<para>Mailing Preference Service (MPS) check</para>
			</listitem>
		</orderedlist>
		<tip>
			<para>Customers need a set of credentials to access the service.
			</para>
		</tip>
		<section>
			<title>
				Address and suppression data sources
			</title>
			<para>The service combines data of several sources, including
				Royal
				Mail, MPS and the Bereavement registry.
			</para>
			<para>The service allows you to get a Price Quotation including
				summary statistics about the expected results before accepting it:
			</para>
			<mediaobject>
				<imageobject>
					<imagedata fileref="analyzer_reference_uksuppression_report.jpg"
						format="JPG" scalefit="1" />
				</imageobject>
			</mediaobject>

		</section>
		<section>
			<title>
				Output
			</title>
			<para>This component outputs name and address fields, in addition to
				the following indicators:
			</para>

			<table>
				<title>DE Address Correction/Suppression output</title>
				<tgroup cols="2">
					<thead>
						<row>
							<entry>Output column</entry>
							<entry>Description</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Address validation status</entry>
							<entry>
								A numeric status code of the record. Following value ranges are
								possible:
								<orderedlist>
									<listitem>
										<para>
											<emphasis>0 - 10</emphasis>
											: The address is valid.
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>11 - 100</emphasis>
											: The address was corrected.
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>101 - 500</emphasis>
											: The address is invalid.
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>700</emphasis>
											: The address was not processed / skipped.
										</para>
									</listitem>
								</orderedlist>
							</entry>
						</row>
						<row>
							<entry>AddressCorrectionMessage</entry>
							<entry>A humanly readable message about the address correction
								outcome.
							</entry>
						</row>
						<row>
							<entry>IsNCOATracked</entry>
							<entry>A boolean flag indicating if the person has moved to a
								new
								address.
							</entry>
						</row>
						<row>
							<entry>IsNCOAFlagged</entry>
							<entry>A boolean flag indicating if the person has moved away
								(with no known new address).
							</entry>
						</row>
						<row>
							<entry>IsDeceased</entry>
							<entry>A boolean flag indicating if the person is deceased.
							</entry>
						</row>
						<row>
							<entry>IsDoNotMail</entry>
							<entry>A boolean flag indicating if the person does not want to
								receive unsolicited mail.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>
		</section>
	</section>

	<section id="US_suppression">
		<title>
			US Address Correction/Suppression
			<inlinemediaobject>
				<imageobject>
					<imagedata fileref="notice_commercial_editions_only.png"
						format="PNG" />
				</imageobject>
			</inlinemediaobject>
		</title>
		<para>This component provides CASS Certified(tm) Address Correction
			and Suppression
			services for the United States of America. Use it to
			check that
			the name and address data you have about people is up to
			date and
			correct. The following Address suppression checks currently
			exist:
		</para>
		<orderedlist>
			<listitem>
				<para>Change of Address check</para>
			</listitem>
			<listitem>
				<para>Deceased check</para>
			</listitem>
			<listitem>
				<para>Do-Not-Mail check</para>
			</listitem>
		</orderedlist>
		<tip>
			<para>Customers need a set of credentials to access the service.
			</para>
		</tip>
		<section>
			<title>
				Address and suppression data sources
			</title>
			<para>The service combines data of several sources, including
				the US
				Postal Service.
			</para>
		</section>
		<section>
			<title>
				Output
			</title>
			<para>This component outputs name and address fields, in addition to
				the following indicators:
			</para>

			<table>
				<title>DE Address Correction/Suppression output</title>
				<tgroup cols="2">
					<thead>
						<row>
							<entry>Output column</entry>
							<entry>Description</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Address validation status</entry>
							<entry>
								A numeric status code of the record. Following value ranges are
								possible:
								<orderedlist>
									<listitem>
										<para>
											<emphasis>0 - 10</emphasis>
											: The address is valid.
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>11 - 100</emphasis>
											: The address was corrected.
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>101 - 500</emphasis>
											: The address is invalid.
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>700</emphasis>
											: The address was not processed / skipped.
										</para>
									</listitem>
								</orderedlist>
							</entry>
						</row>
						<row>
							<entry>AddressCorrectionMessage</entry>
							<entry>A humanly readable message about the address correction
								outcome.
							</entry>
						</row>
						<row>
							<entry>EcoaFootnote</entry>
							<entry>
								An indicator value telling what the outcome of the 'Change of
								Address' check was. The following tokens can occur:
								<orderedlist>
									<listitem>
										<para>
											<emphasis>N</emphasis>
											- No change
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>M</emphasis>
											- The party has a new address
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>K</emphasis>
											- The party has moved away without a new address
										</para>
									</listitem>
								</orderedlist>
								<para>Furthermore the field may have a token representing the
									type of party that was identified:
								</para>
								<orderedlist>
									<listitem>
										<para>
											<emphasis>I</emphasis>
											- Individual
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>F</emphasis>
											- Family
										</para>
									</listitem>
									<listitem>
										<para>
											<emphasis>B</emphasis>
											- Business
										</para>
									</listitem>
								</orderedlist>
							</entry>
						</row>
						<row>
							<entry>IsDeceased</entry>
							<entry>A boolean flag indicating if the person is deceased.
							</entry>
						</row>
						<row>
							<entry>IsDoNotMail</entry>
							<entry>A boolean flag indicating if the person does not want to
								receive unsolicited mail.
							</entry>
						</row>
					</tbody>
				</tgroup>
			</table>
		</section>
	</section>

	<section id="easydq_transformers">
		<title>Name and Address correction</title>
		<para>
			These functions are provided as clients to
			<link xl:href="http://www.easydq.com">EasyDQ.com</link>
			. EasyDQ is an on-demand service for data quality functions.
			DataCleaner provides access to these EasyDQ services,
			but a separate
			account is needed in order
			to take them to any use.
		</para>
		<para>
			Please refer to the
			<link xl:href="http://help.easydq.com/datacleaner">EasyDQ for DataCleaner documentation</link>
			for detailed information about the services provided through
			EasyDQ.
		</para>

		<mediaobject>
			<imageobject>
				<imagedata fileref="transformer_reference_table_easydq_documentation.jpg"
					format="JPG" scalefit="1" />
			</imageobject>
		</mediaobject>

	</section>
</chapter>
