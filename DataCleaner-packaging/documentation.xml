<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<book>
	<title>DataCleaner reference documentation</title>
	<titleabbrev>DataCleaner</titleabbrev>
	<bookinfo>
		<author>
			<personname>
				<firstname>Kasper</firstname>
				<surname>S&#248;rensen</surname>
			</personname>
		</author>
		<edition>1.5-SNAPSHOT</edition>
	</bookinfo>
	<preface id="preface1">
		<title>Preface</title>
		<para>
			Data quality is an issue too often ignored. With almost any type of application,
			datastore or document comes a set of potential data quality pitfalls. DataCleaner
			was founded upon the idea of promoting data quality awareness and bringing it to the
			general public. This required a drastic change from the strategy that commercial data
			quality products are practicing. If we want the general public to be aware of the
			data quality domain, then we have to provide them with the tools to experience it themselves.
		</para>
		<para>
			Although we recognize that dedicated data quality analysis is, and probably
			always will be, a professional niche with a limited audience, we believe that for the most part,
			data quality tools should be free and developed in coorporation and participation between
			those professionals. Since data quality in many ways represents a "common good" and a an
			issue of general interest, we believe in this model of shared ownership and responsibility. This
			is why DataCleaner was founded as an Open Source project and we hope that the implifications of
			the Open Source philosophy will influence the products future in a positive way.
		</para>
	</preface>
	<chapter>
		<title>Introduction to the concepts and features of DataCleaner</title>
		<section>
			<title>What is Data Quality?</title>
			<para>
				The field of data quality has been overseen in the IT-business for a long time. Organizations are
				beginning to feel the pain from inconsistent and flawed systems and interest is being built to support
				the employment of more ambitious goals as to the quality of our data. To illustrate the concept, let's
				imagine an information chain typically centered around the process of building a data warehouse. Even
				though you can employ data quality principles in lots of other scenarios, the data warehouse is the
				archetypical situation since the data warehouse seeks to create a single version of the truth - and
				obviously this truth has to be of high quality!</para>
			<para>Consider the information chain illustrated below.</para>
			<graphic fileref="docs/dataqualityscenarios.png" scale="37" />
			<para>
				Data quality problems can exist at many levels, let's take a couple of examples:
			</para>
			<orderedlist>
				<listitem>
					<para>The users who provide input to the operational systems may have entered invalid, inconsistent or insufficient information.</para>
				</listitem>
				<listitem>
					<para>The data integration specialist is not aware of all the business rules that apply to some fields in the databases.</para>
				</listitem>
				<listitem>
					<para>Some of the operational systems provide data to each other but have done so in a non-historical way, which can make for faulty records.</para>
				</listitem>
				<listitem>
					<para>and so on...</para>
				</listitem>
			</orderedlist>
		</section>
		<section>
			<title>What is Open Source software?</title>
			<para>TODO</para>
		</section>
		<section>
			<title>Data Profiling</title>
			<para>
				The profiler is used to calculate and analyse various important measures based on the values of data.
				In this sense the results of a profiling will always have to be read and contemplated upon by a
				physical person, for example a Database Administrator, BI engineer or similar. The results are
				supposed to help this person determine where to look for data quality problem in the data source -
				to get an impression of the state of the data.
			</para>
		</section>
		<section>
			<title>Data Validation</title>
			<para>
				In contrast to the profiler and the comparator, the validator will give you a result that can be
				interpreted as "good" or "bad", since the validator validates your data. In the validator-mode you
				set up business rules that apply to your data and recieve a result that can be used to fix
				validation errors.
			</para>
		</section>
	</chapter>
	<chapter>
		<title>Installation and configuration</title>
		<para>This chapter covers the installation and configuration of DataCleaner.</para>
		<section>
			<title>Software requirements</title>
			<para>
				DataCleaner requires a Java Runtime Environment (JRE) version 5.0 or higher.
				You can download the JRE at Sun Microsystems Java website:
			</para>
			<para>
				<ulink url="http://java.sun.com/javase/downloads">http://java.sun.com/javase/downloads</ulink>
			</para>
			<para>
				DataCleaner runs in both desktop application mode and command-line interface mode on
				Microsoft Windows Operating Systems, Mac OS X and Linux. For optimal performance, a minimum
				of 1024 megabytes (1 gigabyte) of free memory is recommended.
			</para>
		</section>
		<section>
			<title>Database drivers</title>
			<para>
				Database drivers enable applications to connect to databases. Since DataCleaner was developed using the Java platform, you will need
				Java-based database drivers, so-called JDBC-drivers, to use DataCleaner with your database. JDBC-drivers exist for all major database-types
				so don't worry, it's highly unlikely that you can't use DataCleaner! 
			</para>
			<para>
				To install a driver you need a JAR (Java Archive) file, containing the driver. To obtain the driver there are a few options,
				depicted in the screenshot below:
			</para>
			<graphic fileref="docs/downloaddriver.png" />
			<orderedlist>
				<listitem>
					<para>
						<emphasis>Automatic download and install:</emphasis> Let DataCleaner take care of everything. If your database-type is represented
						in the submenu for this option, then you are in luck - DataCleaner can automatically download the driver and install it for you.
					</para>
				</listitem>
				<listitem>
					<para>
						<emphasis>Visit driver website:</emphasis> Some database-vendors require that you register yourself and other stuff before you are
						allowed to download the driver. DataCleaner provides some handy shortcuts for these vendors websites but we can't do much more to
						easen the process for you. Once you have downloaded the drivers, proceed to the next option, the Local JAR file.
					</para>
				</listitem>
				<listitem>
					<para>
						<emphasis>Local JAR file:</emphasis> When you've aquired the driver file by yourself then you need to pick it out using this option.
						Once you've selected a JAR file from your local harddrive you will be prompted for a driver class name. DataCleaner contains suggestions
						for almost every major database-vendor's driver class name but should it not be provided by DataCleaner you can also manually enter a new
						class name here.
					</para>
				</listitem>
			</orderedlist>
			<para>
				<emphasis>Note:</emphasis> DataCleaner is compatible with <emphasis>any database</emphasis> that complies to two standards: JDBC and SQL 99. Most
				databases are compliant with these standards but some aren't. We are sorry that the application is not compatible with such databases, but if you
				find yourself in such a situation, we strongly suggest you reconsider your choice of database-vendor (because standards, especially in this vital
				area, is a good thing).
			</para>
		</section>
		<section>
			<title>Database connections</title>
			<para>
				To begin with we recommend getting your database connection up and running using the DataCleaner desktop application. From within the
				Profiler, the Validator or the Comparator you can click one of the Open database buttons, which brings up the <emphasis>Open database dialog</emphasis>:
			</para>
			<graphic fileref="docs/opendatabase.png" />
			<para>
				The Open database dialog contains the configuration properties for a database connection. Here are the main ones:
			</para>
			<orderedlist>
				<listitem>
					<para>
						<emphasis>Name:</emphasis> The name field at the top specifies the name of the connection. This is a read-only property since connection
						naming is only possible through the configuration file (more about this later).
					</para>
				</listitem>
				<listitem>
					<para>
						<emphasis>Connection string:</emphasis> The connection string is a vendor-specific string that tells the database driver where and how
						to connect to the database. Notice the small button to the right of the connection string text-field. This button will bring up a drop-down
						list with template for the database drivers that you have installed. When a template is selected the connection string field will be
						<emphasis>automatically filled out</emphasis> but certain parts of the template will still need to be filled out by you (such as the
						network address of your server). You can swap between these parts of the template by pressing the TAB key on your keyboard.
					</para>
				</listitem>
			</orderedlist>
			<para>
				In order to always have easy access to your database we recommend adding it to a configured list of database connections.
				Modify the file <emphasis>datacleaner-config.xml</emphasis> and insert a &lt;bean&gt; XML-element like the ones that
				already exist in there. Here are a few useful examples:
			</para>
			<orderedlist>
				<listitem>
					<para>
						<emphasis>Oracle example</emphasis>
						<literallayout>&lt;bean class="dk.eobjects.datacleaner.gui.model.NamedConnection"&gt;
  &lt;property name="name" value="Oracle database example" /&gt;
  &lt;property name="driverClass" value="oracle.jdbc.OracleDriver" /&gt;
  &lt;property name="connectionString" value="jdbc:oracle:thin:@host:port/service" /&gt;
  &lt;property name="username" value="username" /&gt;
  &lt;property name="password" value="password" /&gt;
&lt;/bean&gt;</literallayout>
					</para>
				</listitem>
				<listitem>
					<para>
						<emphasis>PostgreSQL example</emphasis>
						<literallayout>&lt;bean class="dk.eobjects.datacleaner.gui.model.NamedConnection"&gt;
 &lt;property name="name" value="Postgresql database example" /&gt;
 &lt;property name="driverClass" value="org.postgresql.Driver" /&gt;
 &lt;property name="connectionString" value="jdbc:postgresql://localhost:5432/my_database" /&gt;
 &lt;property name="username" value="username" /&gt;
 &lt;property name="password" value="password" /&gt;
&lt;/bean></literallayout>
					</para>
				</listitem>
				<listitem>
					<para>
						<emphasis>MySQL example</emphasis>
						<literallayout>&lt;bean class="dk.eobjects.datacleaner.gui.model.NamedConnection"&gt;
 &lt;property name="name" value="MySQL database example" /&gt;
 &lt;property name="driverClass" value="com.mysql.jdbc.Driver" /&gt;
 &lt;property name="connectionString" value="jdbc:mysql://localhost:3306" /&gt;
 &lt;property name="catalog" value="my_database" /&gt;
 &lt;property name="username" value="username" /&gt;
 &lt;property name="password" value="password" /&gt;
&lt;/bean&gt;</literallayout>
					</para>
				</listitem>
			</orderedlist>
			<para>
				You can produce your own &lt;bean&gt; XML-elements simply by mapping the connection properties in the Open Database dialog
				within the DataCleaner desktop application to the corresponding &lt;property&gt; XML elements inside the &lt;bean&gt; XML-element.
			</para>
		</section>
		<section>
			<title>Dictionaries</title>
			<para>
				Dictionaries are used for matching or validating entries in your datastore. A dictionary can hold
				a large set of values representing a category of values, for example "valid first names". Dictionaries
				in DataCleaner can be based either on a text-file containing the values or a database column.
			</para>
		</section>
		<section>
			<title>Regular Expressions (regexes)</title>
			<para>
				Regular expressions (or "regexes" in short) are expressions that define a rule for formatting
				text-strings. You can create these expressions yourself or use the online
				<ulink url="http://datacleaner.eobjects.org/regexswap">RegexSwap</ulink> facility which holds a lot
				of reusable expressions for you to use. The regexes can be used in conjunction with the "Regex matcher"
				profile or the "Regex validation" validation rule.
			</para>
		</section>
		<section>
			<title>Optimization techniques</title>
			<para>
				DataCleaner supports a number of different optimization techniques that can be combined in
				order to make your job execute as quick and efficient as possible. Most optimization techniques
				have trade-offs which is why the default configuration of DataCleaner is the most fail-safe configuration
				that works with any type of dataset.
			</para>
			<para>
				Most optimization techniques regard the generation and execution of database-queries. For the Profiler and Validator
				to run it relies on an advanced querying and query-execution framework which supports various ways of processing
				the data in your database.
			</para>
			<para>
				In the base-case DataCleaner will query columns from each profiled/validated table in sequence using a
				very simple SELECT statement:
			</para>
				<blockquote>
					<para>SELECT [col1], [col2] FROM [table]</para>
				</blockquote>
			<para>
				Optimization techniques fall into two categories, which affect this execution process:
			</para>
			<orderedlist>
				<listitem>
					<para><emphasis>Parallel execution</emphasis>, which enables simultanious (as opposed to sequential) execution of queries.</para>
				</listitem>
				<listitem>
					<para><emphasis>Query modification</emphasis>, which optimizes the generated queries for faster individual execution time.</para>
				</listitem>
			</orderedlist>
			<para>
				To access optimization options, click the <emphasis>Profiler Options</emphasis> button in the Profiler or
				<emphasis>Validator Options</emphasis> button in the Validator (only enabled when databases have been opened).
			</para>
			<section>
				<title>Max connections</title>
				<para>
					This is probably the most significant optimization options for any job (that spans multiple queries).
					Depending on this option DataCleaner will open several connections to the same database which can
					be used to feed simultanious execution of independent queries. Running independent queries simultaniously
					provides for a much better utilization of CPU, especially for systems with multiple processors.
				</para>
			</section>
			<section>
				<title>Max simultanious queries per connection</title>
				<para>
					Instead of opening separate connections, you can also enable execution of multiple simultanious queries
					on the same connection(s). Depending on the database-type this may or may not be more efficient than the
					<emphasis>Max connections</emphasis> option above. Used in conjunction with the
					<emphasis>Max connections</emphasis> option, this option enables another layer of simultanious execution
					(ie. if <emphasis>Max connections</emphasis> is set to 3 and <emphasis>Max simultanious queries per connection</emphasis>
					is set to 4, then there can be up to 12 (3x4) queries executing at the same time).
				</para>
				<para>
					Note: Some databases and/or database-drivers doesn't support multiple simultanious
					queries on the same connection, so make sure to test it before relying on absolute compatibility.
				</para>
			</section>
			<section>
				<title>Enabled GROUP BY optimization</title>
				<para>
					This optimization technique modifies the generated queries to include GROUP BY clauses for each queried column. This means that
					if you are analyzing column A, B and C then the generated query will be transformed from:
				</para>
				<blockquote>
					<para>SELECT <emphasis>A, B, C</emphasis> FROM [table]</para>
				</blockquote>
				<para>... to:</para>
				<blockquote>
					<para>SELECT <emphasis>A, B, C, COUNT(*)</emphasis> FROM [table] GROUP BY <emphasis>A, B, C</emphasis></para>
				</blockquote>
				<para>
					What does this accomplish? There are several interesting features of this optimization technique:
				</para>
				<orderedlist>
					<listitem>
						<para>
							<emphasis>Smaller result set size</emphasis> - The result set of the query will be smaller than or (in rare cases) equal
							to the original result set. This means that there is less data to transport from the database to DataCleaner, which means
							less I/O.
						</para>
					</listitem>
					<listitem>
						<para>
							<emphasis>Longer querying time</emphasis> - The generated query is more complex for the database to calculate so more time
							is expected to be consumed from the database which can be bad if it is used by other applications as well.
						</para>
					</listitem>
					<listitem>
						<para>
							<emphasis>Push-down optimization</emphasis> - Some profiles and validation rules performs most of the aggregation that is included
							in the query any way, so doing it in the query is from a "total execution time" perspective probably the most efficient way to do it,
							since aggregation is usually fastest on the database/server-side.
						</para>
					</listitem>
					<listitem>
						<para>
							<emphasis>Unfortunate scenarious</emphasis> - If your tables contain a lot of distinct rows then this is a very bad optimization
							technique! What will happen is that the database will get a huge overhead of comparing rows that are uncomparable and that the
							COUNT-value will be 1 in almost all cases. Thus no improvement is introduced in DataCleaner.
						</para>
					</listitem>
				</orderedlist>
				<para>
					Enabling GROUP BY optimization should be considered carefully and is only recommended if you know that the tables you are
					querying are expected to have a lot of repeated values (ie. a relatively low distinct row count).
				</para>
				<para>
					Note: In DataCleaner 1.0 to 1.4 this option was enabled by default! If you are experiencing poorer performance in
					DataCleaner 1.5+ then this is probably the cause and you should consider turning it on.
				</para>
			</section>
			<section>
				<title>Split queries</title>
				<para>
					Splitting queries is the process of turning a single query into multiple queries that yield the same collective result.
					The easiest way of explaining it is by example. The following query:
				</para>
				<blockquote>
					<para>SELECT name, email FROM persons</para>
				</blockquote>
				<para>
					 ... can be transformed into several queries:
				</para>
				<orderedlist>
					<listitem>
						<para>SELECT name, email FROM persons WHERE age &lt; 20 OR age IS NULL</para>
					</listitem>
					<listitem>
						<para>SELECT name, email FROM persons WHERE age &lt; 40 AND age &gt;= 20</para>
					</listitem>
					<listitem>
						<para>SELECT name, email FROM persons WHERE age &gt;= 40</para>
					</listitem>
				</orderedlist>
				<para>
					As you can see, the result sets of the three generated queries will contain the same collective result as the original query.
					Splitting queries requires a small initial analysis to establish which indexes (the age column in the example above) and which
					index values (1000 in the example above) should be used to create the WHERE-clause(s).
				</para>
				<para>
					So what does splitting queries accomplish? One might think that this is useless processing overhead, but there are scenarios where
					seperate queries can yield a better total execution-time:
				</para>
				<orderedlist>
					<listitem>
						<para>
							<emphasis>Simultanious execution of queries</emphasis> - When used in conjunction with the <emphasis>Max connections</emphasis>
							and/or <emphasis>Max simultanious queries per connection</emphasis> options, splitting queries can introduct parallel execution
							of queries on the same table, which is otherwise not possible.
						</para>
					</listitem>
					<listitem>
						<para>
							<emphasis>Memory management for dysfunctional database drivers</emphasis> - Some database drivers (no names mentioned here) are
							less briliant than others. In developing applications for large resultsets we've found that some drivers haven't really considered
							their data-streaming strategy that well. This means that sometimes when you execute a query that yields a really large result set
							you can potentially be killing your application because it is flooded with data that it has to keep in memory! Splitting queries can
							be seen as a countermeasure to such a problem because the result sets of splitted queries are smaller and thus less memory consuming. 
						</para>
					</listitem>
				</orderedlist>
				<para>
					We recommend enabling Split queries for tables that have a relatively high amount of rows. If for instance you are analyzing three tables with
					10.000, 15.000 and 530.000 rows, then we recommend enabling Split queries for tables with more than 500.000 rows. This will mean that only the
					largest of the three tables will be processed using split queries and thus providing parallel execution of that table's queries, because analysis
					of the first two tables are quickly done with.
				</para>
				<para>
					To learn more about splitting queries, please visit the website of MetaModel, the data-access framework used by DataCleaner, and
					specifically the subsite for the Query Splitter component:
				</para>
				<para>
					<ulink url="http://eobjects.org/metamodel">http://eobjects.org/metamodel</ulink>
				</para>
				<para>
					<ulink url="http://eobjects.org/trac/wiki/QuerySplitter">http://eobjects.org/trac/wiki/QuerySplitter</ulink>
				</para>
			</section>
			<section>
				<title>Enable drill-to-detail in profiler results (Profiler only)</title>
				<para>
					In the Profiler this is an extra optimization technique which regards the content-size of the generated profiler results. Typically, the
					profiler results will include drill-to-details capabilities that enable you to click a measure and view the rows of your database that represent
					the measure. This is very usuful for investigation and interactive analysis.
				</para>
				<para>
					In some scenarios this drill-to-details feature is however not used. Since the metadata about each drill-to-details enabled measure consumes
					memory as well as requires additional processing in the profiles, this feature can be turned off. Turning it off will improve execution time and
					memory consumption, but don't expect a drastic difference.
				</para>
			</section>
		</section>
	</chapter>
	<chapter>
		<title>The DataCleaner desktop application</title>
		<section>
			<title>What is the desktop application used for?</title>
			<para>
				The desktop application is the main entry for the end user to the wonders of DataCleaner. Use the
				desktop application (sometimes referred to as "DataCleaner GUI") to perform datastore investigations,
				explore it and get interactive snapshots of the current datastore state. The desktop application is divided
				into three main task-oriented sub-applications: The Profiler, The Validator and The Comparator.
			</para>
		</section>
		<section>
			<title>The Profiler</title>
			<para>
				They layout of the main Profiler window is depicted in the figure below. It is divided
				into three main sections:
			</para>
			<orderedlist>
				<listitem>
					<para>
						<emphasis>Right: Control buttons</emphasis>. These button let you select your datastore, save
						the Profiler job to file, handle execution options, add profiles to the job and run the job.
					</para>
				</listitem>
				<listitem>
					<para>
						<emphasis>Left: Schema tree</emphasis>. The schema tree is used for datastore browsing. From it
						you can select tables and columns to add to your Profiler job as well as previewing table data.
					</para>
				</listitem>
				<listitem>
					<para>
						<emphasis>Right: Profiles configuration</emphasis>. Each added profile will have it's own tab
						which contains profile-specific configuration options. Most profiles let you select a subset
						of columns to profile from the general data selection specified in the "Data selection" tab. The
						"Metadata" tab contains static metadata about the selected columns, obtained from the datastore
						itself (ie. not profiled metadata).
					</para>
				</listitem>
			</orderedlist>
			<graphic fileref="docs/profiler-window.png" scale="37" />
			<para>
				When you have finished setting up the Profiler, click the "Run profiling" button
				which will bring up the Profiler results window. The results window will let you view
				the execution status browse the results, explore drill-to-details enabled measures (by
				clicking the green arrows) and export the results to file.
			</para>
			<graphic fileref="docs/profiler-results-window.png" scale="37" />
		</section>
		<section>
			<title>The Validator</title>
			<para>TODO: Not yet written. The Profiler documentation is somewhat transferable to the Validator as well.</para>
		</section>
		<section>
			<title>The Comparator</title>
			<para>TODO: Not yet written. </para>
		</section>
	</chapter>
	<chapter>
		<title>The DataCleaner command-line interface</title>
		<section>
			<title>What is the command-line interface used for?</title>
			<para>
				Whereas the desktop application is used for typical desktop work as well as an explorative style of
				work, the command-line interface is used for more repetitive or static jobs. The command-line interface
				is great for keeping track of changes in profiling and validation measures over time, but less appropriate
				for uncovering the initial mysteries of datastore contents.
			</para>
		</section>
		<section>
			<title>Running a DataCleaner job</title>
			<para>
				DataCleaner supports command-line execution using the runjob tool, aka. runjob.sh (on linux/unix platforms)
				and runjob.cmd (on windows platforms). Calling the runjob command with no parameters yields the following
				result:
			</para>
			<blockquote>
				<literallayout>usage: runjob
Use this command line tool to execute DataCleaner jobs (.dcp or .dcv files)
 -?,--help                     show this help message
 -c,--named-connection &lt;arg&gt;   Named database connection to use (replaces the connection specified
                               in the input-file)
 -i,--input-file &lt;arg&gt;         input file path (DataCleaner Profiler (.dcp) or DataCleaner Validator
                               (.dcv) file)
 -o,--output-file &lt;arg&gt;        output file path (should match the output type)
 -ow,--overwrite               overwrite output-file if it already exists
 -t,--output-type &lt;arg&gt;        output type (xml|csv|customClassName)</literallayout>
			</blockquote>
			<para>
				The only required parameter is the <emphasis>input-file</emphasis> parameter, which tells DataCleaner which .dcp or .dcv file
				to execute (using the Profiler or Validator respectively). The following sections contains explanations for the parameters.
			</para>
		</section>
		<section>
			<title>Output formats</title>
			<para>
				As described above you can specify output format by using the <emphasis>output-type</emphasis>
				CLI-parameter. In the following sections we will describe the default output type, XML, an alternative
				CSV output format and how to extend DataCleaner with custom output formats.
			</para>
			<section>
				<title>XML and CSV output formats</title>
				<para>
					The default <emphasis>output-type</emphasis> of DataCleaner CLI is an XML format suited
					for persisting all the available information about the job. Most users will find the XML
					output format satisfying and sufficient. 
				</para>
				<para>
					The CSV <emphasis>output-type</emphasis> will generate multiple files, one for each profiled
					or validated table. If an exception occurs during execution, the csv file will contain only
					the stack trace from the exception. The CSV output format is comma-based, with double-quotes
					around values. 
				</para>
			</section>
			<section>
				<title>Using named connections in command-line execution</title>
				<para>
					It is possible to overwrite the connection metadata written in the .dcp or .dcv files to be
					executed using the runjob tool. To to this, use the <emphasis>named-connection</emphasis>
					parameter to specify a connection name to use (defined in the datacleaner-config.xml configuration
					file).
				</para>
			</section>
			<section>
				<title>Custom output formats</title>
				<para>
					With little effort it is possible to extend DataCleaner with your own output formats
					for the DataCleaner CLI application. To do this you will need a little Java development
					skills but the complexity level is kept at a minimum.
				</para>
				<para>
					To create your own type of output you will have to create a Java class that implements the
					<emphasis>dk.eobjects.datacleaner.export.IResultExporter</emphasis> interface.
					In the interface a PrintWriter instance is offered to support textual formating of the results
					but you are not limited to this kind of format. If you need it, you can create your own
					class that saves the results to a database, sends them as web-service calls. All this is leveraged
					by the standard Java class library which is at your disposal.
				</para>
				<para>
					When you're done developing your custom IResultExplorer implementing class,
					using it from the command line requires two things:
				</para>
				<orderedlist>
					<listitem>
						<para>
							Adding your class to the class path. This can be done either by placing
							your java .class file(s) relative to DataCleaner's root-directory or if you prefer
							to bundle the .class file(s) inside a JAR-file, you can add the JAR-file to the
							classpath by editing runjob.cmd/.sh to include it when calling the java command.
						</para>
					</listitem>
					<listitem>
						<para>
							Providing the class as the <emphasis>--output-type</emphasis>
							parameter when you call DataCleaner:
						</para>
						<blockquote>
							<para>runjob --output-type <emphasis>com.mycompany.MyResultExporter</emphasis></para>
						</blockquote>
					</listitem>
				</orderedlist>
			</section>
		</section>
		<section>
			<title>Scheduling jobs</title>
			<para>
				DataCleaner does not ship with a scheduler. This is a deliberate choice because there are so many
				priorities that go into selecting the right scheduler. For people who just want a simple scheduling
				of DataCleaner tasks, we recommend using <emphasis>cron</emphasis> for linux/unix machines and 
				<emphasis>scheduled tasks</emphasis> for windows machines.
			</para>
			<para>
				If you're applying DataCleaner to an enterprise data warehousing environment be aware that most
				enterprise suites have their own schedulers and applying DataCleaner jobs to their workflows should
				be quite possible.
			</para>
		</section>
	</chapter>
	<chapter>
		<title>Developers guide</title>
		<section>
			<title>Java API reference documentation</title>
			<para>The Java API reference documentation ("javadoc") is available at the following URL:</para>
			<para>
				<ulink url="http://eobjects.org/datacleaner/apidocs/current">http://eobjects.org/datacleaner/apidocs/current</ulink>
			</para>
		</section>
		<section>
			<title>An introduction to the architecture of DataCleaner</title>
			<para>TODO: Please refer to <ulink url="http://eobjects.org/trac/wiki/DataCleanerDesign">http://eobjects.org/trac/wiki/DataCleanerDesign</ulink></para>
		</section>
		<section>
			<title>Understanding the DataCleaner ecosystem</title>
			<para>TODO: Please refer to <ulink url="http://eobjects.org/trac/wiki/CreatingPatches">http://eobjects.org/trac/wiki/CreatingPatches</ulink></para>
		</section>
		<section>
			<title>Developing custom profiles</title>
			<para>TODO: Please refer to <ulink url="http://eobjects.org/trac/wiki/DataCleanerDesign">http://eobjects.org/trac/wiki/DataCleanerDesign</ulink></para>
		</section>
		<section>
			<title>RegexSwap RESTful web services</title>
			<para>
				The <ulink url="http://datacleaner.eobjects.org/regexswap">RegexSwap</ulink> was made with
				the broadest audience in mind, not just users of DataCleaner. For this reason we have applied
				some RESTful web services to service anyone who wants to access the online repository of
				regular expressions.
			</para>
			<para>
				The RESTful web services are:
			</para>
			<section>
				<title>Categories</title>
				<para>Provides a list of all the categories in the RegexSwap</para>
				<para>URL: <ulink url="http://datacleaner.eobjects.org/ws/categories">http://datacleaner.eobjects.org/ws/categories</ulink></para>
			</section>
			<section>
				<title>Regexes in category</title>
				<para>Provides a list of regexes in a specified category</para>
				<para>URL template: http://datacleaner.eobjects.org/ws/regexes/[category name]</para>
				<para>Example URL: <ulink url="http://datacleaner.eobjects.org/ws/regexes/programming">http://datacleaner.eobjects.org/ws/regexes/programming</ulink></para>
			</section>
			<section>
				<title>Regex details</title>
				<para>Provides detail data about a single regex.</para>
				<para>URL template: http://datacleaner.eobjects.org/ws/regex/[regex name]</para>
				<para>Example URL: <ulink url="http://datacleaner.eobjects.org/ws/regex/Java%20class%20name">http://datacleaner.eobjects.org/ws/regex/Java%20class%20name</ulink></para>
			</section>
		</section>
	</chapter>
</book>
